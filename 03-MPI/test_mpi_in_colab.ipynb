{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Running MPI in Google colab\n",
        "A sample workflow on how to run and test mpi code using google colab. Caution on the below workflow as it involves over subcribing a processor. This can be used as a last resort for developing MPI applications. Although technically you can run almost anything that you do in linux terminal, understand its limitations."
      ],
      "metadata": {
        "id": "Mtan8gn_PzOK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5cBTx-fIypD"
      },
      "outputs": [],
      "source": [
        "! mpicxx --version"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample MPI code"
      ],
      "metadata": {
        "id": "2TyR-BYzQrVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test_mpi.cpp\n",
        "#include<iostream>\n",
        "#include<mpi.h>\n",
        "int main()\n",
        "{\n",
        "  MPI_Init(NULL, NULL); // Initialised MPI\n",
        "  // Find my (processors') Unique ID - (also called as rank)\n",
        "  int my_rank;\n",
        "  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n",
        "  // Find the total number of processors\n",
        "  int world_size;\n",
        "  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n",
        "  MPI_Barrier(MPI_COMM_WORLD);\n",
        "  std::cout << \"My rank is \" << my_rank << \" among \" << world_size << \" Processors..\" << std::endl;\n",
        "  MPI_Finalize(); // MPI Terminated\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "vPMwBLrtJ8wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compiling the MPI code"
      ],
      "metadata": {
        "id": "8A_89xp7RHVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mpicxx test_mpi.cpp -o test"
      ],
      "metadata": {
        "id": "P8TQAIsXKp5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the MPI code\n",
        "1. --use-hwthread-cpus ensures each mpi process runs on single code.\n",
        "2. -oversubscribe is MPI processes run on same processor although the parallel region nature is preserved."
      ],
      "metadata": {
        "id": "g55jqI4ERNIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mpiexec -n 2 --allow-run-as-root --use-hwthread-cpus ./test"
      ],
      "metadata": {
        "id": "7lbbXZA7Q9wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mpirun -n 8 --allow-run-as-root -oversubscribe --use-hwthread-cpus ./test"
      ],
      "metadata": {
        "id": "8hP2uDM_KyXi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}